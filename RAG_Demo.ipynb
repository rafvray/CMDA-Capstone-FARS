{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180d5503",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb13733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Demo tools loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"✅ Demo tools loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba7569",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6132908c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model 'sentence-transformers/all-MiniLM-L6-v2' is loaded and ready.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the open-source embedding model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "print(f\"✅ Model '{model_name}' is loaded and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12347d83",
   "metadata": {},
   "source": [
    "### **Text-to-Vector**\n",
    "*(Main component of RAG)*\n",
    "\n",
    "Here is a plain English question being converted to numeric vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "693823c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Your Question ---\n",
      "'How many accidents in Virginia involved alcohol?'\n",
      "\n",
      "--- Becomes a 'Vector' (a list of numbers) ---\n",
      "\n",
      "First 5 numbers of the vector:\n",
      "[0.08255327 0.00739611 0.0154931  0.02287057 0.03947461]\n",
      "\n",
      "Total length of the vector: 384\n"
     ]
    }
   ],
   "source": [
    "# Here is a plain English question\n",
    "question = \"How many accidents in Virginia involved alcohol?\"\n",
    "\n",
    "# Let's run it through the model\n",
    "vector = embeddings.embed_query(question)\n",
    "\n",
    "print(\"--- Your Question ---\")\n",
    "print(f\"'{question}'\")\n",
    "\n",
    "print(\"\\n--- Becomes a 'Vector' (a list of numbers) ---\")\n",
    "print(\"\\nFirst 5 numbers of the vector:\")\n",
    "print(np.array(vector)[:5])\n",
    "print(f\"\\nTotal length of the vector: {len(vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd89bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedded two more sentences to compare.\n"
     ]
    }
   ],
   "source": [
    "# Now, let's create two other sentences to compare\n",
    "text_similar = \"What is the number of drunk driving crashes in VA?\"\n",
    "text_dissimilar = \"What's the weather like in California?\"\n",
    "\n",
    "# Let's embed these as well\n",
    "vector_similar = embeddings.embed_query(text_similar)\n",
    "vector_dissimilar = embeddings.embed_query(text_dissimilar)\n",
    "\n",
    "print(\"✅ Embedded two more sentences to compare.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea1da660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Similarity Results (1.0 = identical) ---\n",
      "Similarity to 'drunk driving': 0.8079\n",
      "Similarity to 'California weather': 0.2156\n"
     ]
    }
   ],
   "source": [
    "# We need to reshape the vectors for the function\n",
    "v_question = np.array(vector).reshape(1, -1)\n",
    "v_similar = np.array(vector_similar).reshape(1, -1)\n",
    "v_dissimilar = np.array(vector_dissimilar).reshape(1, -1)\n",
    "\n",
    "# Now, let's use 'cosine similarity' to see how \"close\" they are.\n",
    "# A score of 1.0 is a perfect match.\n",
    "sim_similar = cosine_similarity(v_question, v_similar)[0][0]\n",
    "sim_dissimilar = cosine_similarity(v_question, v_dissimilar)[0][0]\n",
    "\n",
    "print(\"--- Similarity Results (1.0 = identical) ---\")\n",
    "print(f\"Similarity to 'drunk driving': {sim_similar:.4f}\")\n",
    "print(f\"Similarity to 'California weather': {sim_dissimilar:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c937c0ff",
   "metadata": {},
   "source": [
    "### **Retreive Data from Your SQL Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07857e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG tools loaded. We will build this from scratch.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "print(\"✅ RAG tools loaded. We will build this from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24319815",
   "metadata": {},
   "source": [
    "### **Load Your FARS Data**\n",
    "\n",
    "Loading small, fast sample from your CSV to act as our \"database.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306f096e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded sample of FARS 'accident_master.csv'.\n",
      "Here's a preview:\n",
      "   ST_CASE  YEAR  STATE  FATALS\n",
      "0    10001  1975      1       1\n",
      "1    10002  1975      1       1\n",
      "2    10003  1975      1       1\n",
      "3    10004  1975      1       1\n",
      "4    10005  1975      1       1\n"
     ]
    }
   ],
   "source": [
    "# Load a sample of your CSV data\n",
    "df = pd.read_csv(\"/Users/rafaelviray/Documents/FARS/Datasets & Ingestion Scripts/accident_master.csv\", nrows=50)\n",
    "\n",
    "print(\"✅ Loaded sample of FARS 'accident_master.csv'.\")\n",
    "print(\"Here's a preview:\")\n",
    "print(df[['ST_CASE', 'YEAR', 'STATE', 'FATALS']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d98d5",
   "metadata": {},
   "source": [
    "### **Building the \"R\" (Retrieval) Component**\n",
    "\n",
    "This is the core **\"technical content\"**. We serialize and index the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11722c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serializing CSV rows into text snippets...\n",
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/xxvzrlmd0wddkflz97l7nq7r0000gn/T/ipykernel_93503/2808096897.py:16: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building live vector store... (This is the 'R' in RAG)\n",
      "✅ Vector store is live.\n"
     ]
    }
   ],
   "source": [
    "# 1. Serialize: Convert rows into simple \"documents\"\n",
    "print(\"Serializing CSV rows into text snippets...\")\n",
    "documents = []\n",
    "for _, row in df.iterrows():\n",
    "    # Create a simple text snippet for each accident\n",
    "    content = (\n",
    "        f\"Accident case {row['ST_CASE']} took place in {row['STATENAME']} \"\n",
    "        f\"(state code {row['STATE']}) in the year {row['YEAR']}. \"\n",
    "        f\"This incident resulted in {row['FATALS']} fatalities.\"\n",
    "    )\n",
    "    doc = Document(page_content=content, metadata={\"st_case\": row['ST_CASE']})\n",
    "    documents.append(doc)\n",
    "\n",
    "# 2. Index: Load the embedding model and build the vector store\n",
    "print(\"Loading embedding model...\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Building live vector store... (This is the 'R' in RAG)\")\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"✅ Vector store is live.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b318fb0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec0a98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/xxvzrlmd0wddkflz97l7nq7r0000gn/T/ipykernel_93503/2675436897.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Asking the 'Dumb' LLM ---\n",
      "QUESTION: How many fatalities were in accident case 47157?\n",
      "\n",
      "--- RESPONSE (Hallucination) ---\n",
      "content='I\\'m a large language model, I don\\'t have access to specific information about accident cases, including the number of fatalities. Additionally, I am not aware of any publicly available information on an accident case with the identifier \"47157\". If you\\'re referring to a specific incident or accident report, please provide more context or details so I can help you better.' additional_kwargs={} response_metadata={'model': 'llama3', 'created_at': '2025-11-05T20:40:10.038412Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 12357539666, 'load_duration': 7508519708, 'prompt_eval_count': 21, 'prompt_eval_duration': 1378666542, 'eval_count': 73, 'eval_duration': 3446750332} id='run--5888b6d6-d308-4f04-ab10-8f6f43221f8e-0'\n"
     ]
    }
   ],
   "source": [
    "# Initialize our open-source LLM\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "# This is a specific question from our CSV data\n",
    "question = \"How many fatalities were in accident case 47157?\"\n",
    "\n",
    "print(f\"--- Asking the 'Dumb' LLM ---\")\n",
    "print(f\"QUESTION: {question}\\n\")\n",
    "\n",
    "# We invoke the LLM *without* RAG. It will fail.\n",
    "response = llm.invoke(question)\n",
    "\n",
    "print(\"--- RESPONSE (Hallucination) ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1449d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Full RAG Chain is built.\n",
      "This chain will now RETRIEVE, AUGMENT, and GENERATE.\n"
     ]
    }
   ],
   "source": [
    "# 1. Augment: We create a prompt template\n",
    "# It will be \"augmented\" with the context we retrieve\n",
    "template = \"\"\"\n",
    "Answer the question based ONLY on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 2. Generate: We build the full chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# This chain links all our steps together\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✅ Full RAG Chain is built.\")\n",
    "print(\"This chain will now RETRIEVE, AUGMENT, and GENERATE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd33554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Asking the 'Smart' RAG Chain ---\n",
      "QUESTION: How many fatalities were in accident case 47157?\n",
      "\n",
      "--- RESPONSE (Grounded in FARS) ---\n",
      "Based on the provided context, there is no information about accident case 47157. Therefore, it is not possible to determine the number of fatalities for this accident case. The only available information pertains to cases 10014, 10015, 10026, and 10033.\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Asking the 'Smart' RAG Chain ---\")\n",
    "print(f\"QUESTION: {question}\\n\")\n",
    "\n",
    "# Invoke the full RAG chain\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "print(\"--- RESPONSE (Grounded in FARS) ---\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fars_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
