{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180d5503",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb13733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Demo tools loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(\"✅ Demo tools loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba7569",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6132908c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model 'sentence-transformers/all-MiniLM-L6-v2' is loaded and ready.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the open-source embedding model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "print(f\"✅ Model '{model_name}' is loaded and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12347d83",
   "metadata": {},
   "source": [
    "### **Text-to-Vector**\n",
    "*(Main component of RAG)*\n",
    "\n",
    "Here is a plain English question being converted to numeric vector.\n",
    "\n",
    "**\"How many accidents in Virginia involved alcohol?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693823c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many accidents in Virginia involved alcohol?\"\n",
    "vector = embeddings.embed_query(question)\n",
    "\n",
    "print(\"--- Your Question ---\")\n",
    "print(f\"'{question}'\")\n",
    "\n",
    "print(\"\\n--- Becomes a 'Vector' (a list of numbers) ---\")\n",
    "print(np.array(vector))\n",
    "print(f\"\\nTotal length of the vector: {len(vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d5c82",
   "metadata": {},
   "source": [
    "#### **Now, let's create two other sentences to compare**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_similar = \"What is the number of drunk driving crashes in VA?\"\n",
    "text_dissimilar = \"What's the weather like in California?\"\n",
    "\n",
    "vector_similar = embeddings.embed_query(text_similar)\n",
    "vector_dissimilar = embeddings.embed_query(text_dissimilar)\n",
    "\n",
    "\n",
    "print(\"✅ Embedded two more sentences to compare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87382253",
   "metadata": {},
   "source": [
    "#### Now, let's use **'cosine similarity'** to see how **\"close\"** they are.\n",
    "\n",
    "#### A score of **1.0** is a perfect match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1da660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to reshape the vectors for the function\n",
    "v_question = np.array(vector).reshape(1, -1)\n",
    "v_similar = np.array(vector_similar).reshape(1, -1)\n",
    "\n",
    "# Using cosine similarity to compare\n",
    "v_dissimilar = np.array(vector_dissimilar).reshape(1, -1)\n",
    "sim_similar = cosine_similarity(v_question, v_similar)[0][0]\n",
    "sim_dissimilar = cosine_similarity(v_question, v_dissimilar)[0][0]\n",
    "\n",
    "\n",
    "print(\"--- Similarity Results ---\")\n",
    "print(f\"Similarity to 'drunk driving': {sim_similar:.4f}\")\n",
    "print(f\"Similarity to 'California weather': {sim_dissimilar:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c937c0ff",
   "metadata": {},
   "source": [
    "### **Retreive Data from Your SQL Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07857e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "print(\"✅ RAG tools loaded. We will build this from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24319815",
   "metadata": {},
   "source": [
    "### **Load Your FARS Data**\n",
    "\n",
    "Loading small, fast sample from your CSV to act as our \"database.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample of your CSV data\n",
    "df = pd.read_csv(\"/Users/rafaelviray/Documents/FARS/Datasets & Ingestion Scripts/accident_master.csv\", nrows=50)\n",
    "\n",
    "print(\"✅ Loaded sample of FARS 'accident_master.csv'.\")\n",
    "print(\"Here's a preview:\")\n",
    "print(df[['ST_CASE', 'YEAR', 'STATE', 'FATALS']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d98d5",
   "metadata": {},
   "source": [
    "### **Building the \"R\" (Retrieval) Component**\n",
    "\n",
    "This is the core **\"technical content\"**. We serialize and index the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11722c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Serialize: Convert rows into simple \"documents\"\n",
    "print(\"Serializing CSV rows into text snippets...\")\n",
    "documents = []\n",
    "for _, row in df.iterrows():\n",
    "    # Create a simple text snippet for each accident\n",
    "    content = (\n",
    "        f\"Accident case {row['ST_CASE']} took place in {row['STATENAME']} \"\n",
    "        f\"(state code {row['STATE']}) in the year {row['YEAR']}. \"\n",
    "        f\"This incident resulted in {row['FATALS']} fatalities.\"\n",
    "    )\n",
    "    doc = Document(page_content=content, metadata={\"st_case\": row['ST_CASE']})\n",
    "    documents.append(doc)\n",
    "\n",
    "# 2. Index: Load the embedding model and build the vector store\n",
    "print(\"Loading embedding model...\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Building live vector store... (This is the 'R' in RAG)\")\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"✅ Vector store is live.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b318fb0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our open-source LLM\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "# This is a specific question from our CSV data\n",
    "question = \"How many fatalities were in accident case 47157?\"\n",
    "\n",
    "print(f\"--- Asking the 'Dumb' LLM ---\")\n",
    "print(f\"QUESTION: {question}\\n\")\n",
    "\n",
    "# We invoke the LLM *without* RAG. It will fail.\n",
    "response = llm.invoke(question)\n",
    "\n",
    "print(\"--- RESPONSE (Hallucination) ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1449d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Augment: We create a prompt template\n",
    "# It will be \"augmented\" with the context we retrieve\n",
    "template = \"\"\"\n",
    "Answer the question based ONLY on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 2. Generate: We build the full chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# This chain links all our steps together\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✅ Full RAG Chain is built.\")\n",
    "print(\"This chain will now RETRIEVE, AUGMENT, and GENERATE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd33554",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Asking the 'Smart' RAG Chain ---\")\n",
    "print(f\"QUESTION: {question}\\n\")\n",
    "\n",
    "# Invoke the full RAG chain\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "print(\"--- RESPONSE (Grounded in FARS) ---\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fars_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
